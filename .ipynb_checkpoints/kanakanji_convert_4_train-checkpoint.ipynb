{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9Srn_cEx1fa"
   },
   "outputs": [],
   "source": [
    "# !pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYk_Je9Be2sS"
   },
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "\n",
    "from utility import load_train_data\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ct8_vGQFe2sX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version:  1.13.0-dev20190111\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"tf version: \", tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcc8Q5KVZgLi"
   },
   "outputs": [],
   "source": [
    "import random as rn\n",
    "\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TufQU1Nee2sa"
   },
   "outputs": [],
   "source": [
    "# prepare train data\n",
    "\n",
    "def parse_file(file):\n",
    "    for line in file:\n",
    "        line = line.rstrip('\\n')\n",
    "        sentence = line.split(' ')\n",
    "        yield sentence\n",
    "\n",
    "# TODO: current method does not allow the model to learn boundary beyond bigram.\n",
    "def adjust_size(sentences, sentence_size):\n",
    "    # Increment sentence size for shifting output later\n",
    "    sentence_size_plus = sentence_size + 1\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Insert BOS = Beginning Of Sentence\n",
    "        sentence.insert(0, '_BOS/_BOS')\n",
    "\n",
    "        # Split long sentence allowing overlap of 1 word\n",
    "        while len(sentence) >= sentence_size_plus:\n",
    "            yield sentence[:sentence_size_plus]\n",
    "            sentence = sentence[sentence_size:]\n",
    "\n",
    "        # Do not yield EOS-only sentence\n",
    "        if sentence:\n",
    "            # Insert EOS = End Of Sentence\n",
    "            sentence.append('_EOS/_EOS')\n",
    "\n",
    "            if len(sentence) < sentence_size_plus:\n",
    "                # Padding sentence to make its size sentence_size_plus\n",
    "                sentence += ['_PAD/_PAD'] * (sentence_size_plus - len(sentence))\n",
    "            yield sentence\n",
    "        \n",
    "\n",
    "def create_vocabulary(sentences, vocabulary_size):\n",
    "    # Create list of words indexed by word ID\n",
    "    counter = Counter(word for words in sentences for word in words)\n",
    "    most_common = counter.most_common(vocabulary_size - 1)\n",
    "    vocabulary = [word for word, count in most_common]\n",
    "    vocabulary.insert(0, '_UNK/_UNK')\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def convert_to_ids(sentences, vocabulary):\n",
    "    dictionary = dict((word, word_id) for word_id, word in enumerate(vocabulary))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_ids = []\n",
    "\n",
    "        for word in sentence:\n",
    "            if word in dictionary:\n",
    "                word_id = dictionary[word]\n",
    "            else:\n",
    "                word_id = dictionary['_UNK/_UNK']\n",
    "            word_ids.append(word_id)\n",
    "\n",
    "        yield word_ids\n",
    "\n",
    "        \n",
    "# TODO: current batching ignores sentences that does't fit into last batch.\n",
    "def create_batches(sentences, batch_size):\n",
    "    all_batches = int(len(sentences) / batch_size)\n",
    "\n",
    "    for i in range(all_batches):\n",
    "        batch_sentences = sentences[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_input = []\n",
    "        batch_output = []\n",
    "\n",
    "        for sentence in batch_sentences:\n",
    "            # Shift sentence by 1 time step\n",
    "            input_ = sentence[:-1]\n",
    "            output_ = sentence[1:]\n",
    "\n",
    "            batch_input.append(input_)\n",
    "            batch_output.append(output_)\n",
    "\n",
    "        yield batch_input, batch_output    \n",
    "\n",
    "        \n",
    "def create_pair(sentences):\n",
    "    \n",
    "    print(\"sentences count: \", len(sentences))\n",
    "#     print(sentences)\n",
    "    \n",
    "    input_list = []\n",
    "    output_list = []\n",
    "    \n",
    "    i=0\n",
    "    for sentence in sentences:\n",
    "#         print(\"aaa i: {}\", i)\n",
    "        input_ = sentence[:-1]\n",
    "        output_ = sentence[1:]\n",
    "        \n",
    "        input_list.append(input_)\n",
    "        output_list.append(output_)\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "    return input_list, output_list\n",
    "        \n",
    "def save_metadata(model_directory, vocabulary):\n",
    "    # Create directory if not exists\n",
    "    if not os.path.exists(model_directory):\n",
    "        os.makedirs(model_directory)\n",
    "\n",
    "#     # Save settings\n",
    "#     settings_path = os.path.join(model_directory, 'settings.json')\n",
    "#     with open(settings_path, 'w') as settings_file:\n",
    "#         json.dump(vars(args), settings_file, indent=4)\n",
    "\n",
    "    # Save vocabulary\n",
    "    vocabulary_path = os.path.join(model_directory, 'vocabulary.txt')\n",
    "    with open(vocabulary_path, 'w') as vocabulary_file:\n",
    "        vocabulary_file.write('\\n'.join(vocabulary))\n",
    "        \n",
    "        \n",
    "def load_train_data(dataset_name, sentence_size, vocabulary_size, batch_size, model_directory ):\n",
    "\n",
    "    sentences = parse_file(open(dataset_name))\n",
    "    sentences = list(adjust_size(sentences, sentence_size))\n",
    "    vocabulary = create_vocabulary(sentences, vocabulary_size)\n",
    "    sentences = list(convert_to_ids(sentences, vocabulary))\n",
    "#     train_data = list(create_batches(sentences, batch_size))\n",
    "    save_metadata(model_directory, vocabulary)\n",
    "    \n",
    "    # target\n",
    "    input_x, target_y = create_pair(sentences)\n",
    "    \n",
    "    return input_x, target_y\n",
    "#     return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZZo5EHvme2sd",
    "outputId": "9f6c35b0-2191-4038-91db-eea77a298883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences count:  80103\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess training data\n",
    "\n",
    "dataset_name = \"wiki_dataset_mecab_80000.txt\"\n",
    "sentence_size = 30\n",
    "BATCH_SIZE = 64\n",
    "# BATCH_SIZE = 5\n",
    "batch_size = BATCH_SIZE\n",
    "vocabulary_size = 50000\n",
    "model_directory = \"models\"\n",
    "\n",
    "\n",
    "input_x, target_y = load_train_data(dataset_name, sentence_size, vocabulary_size, BATCH_SIZE, model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "NhTydzl-e2sj",
    "outputId": "33de1813-31c2-4a25-b62f-9f4df12f5af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_x, count: {} 80103\n",
      "target_y, count: {} 80103\n",
      "input_x.0:  [3, 37098, 12, 9, 7833, 16536, 12, 6, 131, 16, 1471, 10, 19, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "target_y.0:  [37098, 12, 9, 7833, 16536, 12, 6, 131, 16, 1471, 10, 19, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"input_x, count: {}\", len(input_x))\n",
    "print(\"target_y, count: {}\", len(target_y))\n",
    "\n",
    "print(\"input_x.0: \", input_x[0])\n",
    "print(\"target_y.0: \", target_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VWeO2Fwe2so"
   },
   "outputs": [],
   "source": [
    "# parepare dataset\n",
    "\n",
    "# BUFFER_SIZE = len(input_x)\n",
    "BUFFER_SIZE = 50000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_x, target_y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "GbrQbX3ne2ss",
    "outputId": "fff96071-b819-4e84-d6f2-d72b1175a937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/koohashi/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "input_x_0: [   3   48   42    5 2936   22 9774   16   33 1561    9 1040   11   24\n",
      "   27    4    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1] (30,)\n",
      "output_y_0: [  48   42    5 2936   22 9774   16   33 1561    9 1040   11   24   27\n",
      "    4    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1] (30,)\n"
     ]
    }
   ],
   "source": [
    "# # dataset.take(1)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    \n",
    "    input_x_0 = input_example.numpy()[0]\n",
    "    print(\"input_x_0:\", input_x_0, input_x_0.shape)\n",
    "    output_y_0 = target_example.numpy()[0]\n",
    "    print(\"output_y_0:\", output_y_0, output_y_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04AyUiMK3EPr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tK2sm9LN3EMu"
   },
   "outputs": [],
   "source": [
    "# if tf.test.is_gpu_available():\n",
    "#   rnn = tf.keras.layers.CuDNNGRU\n",
    "# else:\n",
    "#   import functools\n",
    "#   rnn = functools.partial(\n",
    "#     tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "\n",
    "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#   model = tf.keras.Sequential([\n",
    "      \n",
    "#     tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "#                               batch_input_shape=[batch_size, None]),\n",
    "#     rnn(rnn_units,\n",
    "#         return_sequences=True, \n",
    "#         recurrent_initializer='glorot_uniform',\n",
    "#         stateful=True),\n",
    "      \n",
    "#     tf.keras.layers.Dense(vocab_size)\n",
    "      \n",
    "#   ])\n",
    "#   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kql7HyL3EA2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OtFy0MBNF1e0"
   },
   "outputs": [],
   "source": [
    "# prepare model No.1\n",
    "\n",
    "\n",
    "# def gru(units, backword_flg):\n",
    "#     # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "#     # the code automatically does that.\n",
    "#     if tf.test.is_gpu_available():\n",
    "#         return tf.keras.layers.CuDNNGRU(units, \n",
    "#                                         return_sequences=True, \n",
    "#                                         return_state=True, \n",
    "#                                         recurrent_initializer='glorot_uniform',\n",
    "#                                         go_backwards=backword_flg)\n",
    "#     else:\n",
    "#         return tf.keras.layers.GRU(units, \n",
    "#                                    return_sequences=True, \n",
    "#                                    return_state=True, \n",
    "#                                    recurrent_activation='sigmoid', \n",
    "#                                    recurrent_initializer='glorot_uniform',\n",
    "#                                    go_backwards=backword_flg)\n",
    "\n",
    "def gru(units, backword_flg):\n",
    "    # should use GRU anyway....\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform',\n",
    "                               go_backwards=backword_flg)\n",
    "    \n",
    "    \n",
    "class KanaKanjiModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "        super(KanaKanjiModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_units = rnn_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_shape=(30,))\n",
    "        self.gru_f = gru(self.rnn_units, False)\n",
    "        self.gru_b = gru(self.rnn_units, True)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "    def call(self, x, hidden, training):\n",
    "        \n",
    "        # print(\"input.shape: \", x.shape)\n",
    "        \n",
    "        \n",
    "        x = self.embedding(x)\n",
    "         \n",
    "        output, state = self.gru_f(x, initial_state = hidden)\n",
    "        \n",
    "        output = self.dropout(output, training=training)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        # print(\"output.shape: \", output.shape)\n",
    "        # print(\"state.shape: \", state.shape)\n",
    "        \n",
    "        \n",
    "        # return output, state, 0\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.rnn_units))\n",
    "      \n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # You need to override this function if you want to use the subclassed model\n",
    "        # as part of a functional-style model.\n",
    "        # Otherwise, this method is optional.\n",
    "        \n",
    "        print(\"input_shape: \", input_shape)\n",
    "        \n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.num_classes\n",
    "        \n",
    "        return tf.TensorShape([tf.TensorShape([-1, 30, 50000]), tf.TensorShape([-1, 30, 400])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-oglZ4Ka5vN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOE5rPcje2sy"
   },
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 400\n",
    "embedding_dim = hidden_size\n",
    "\n",
    "model = KanaKanjiModel(vocabulary_size, embedding_dim, hidden_size, BATCH_SIZE)\n",
    "# model = build_model(vocabulary_size, embedding_dim, hidden_size, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNl8mpWQDtJO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cld5YPsx6kl_"
   },
   "outputs": [],
   "source": [
    "# it's not trivial to wrap already complicated model!!!\n",
    "\n",
    "# x_input = tf.keras.layers.Input(shape=(30,))\n",
    "# model1 = KanaKanjiModel(vocabulary_size, embedding_dim, hidden_size, BATCH_SIZE)(x_input)\n",
    "# model = tf.keras.Model(inputs=x_input, outputs=model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fTwLxO_75_Z"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXmv4IuH3_yx"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "p4nDA_7me2s0",
    "outputId": "2bbbfbf3-9de5-43d4-daa6-94cd296bf45f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0-dev20190111'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pl15bZZ3e2s5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# custom model don't have summary\n",
    "# model.summary()\n",
    "\n",
    "# speed up\n",
    "model.call = tf.contrib.eager.defun(model.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0IFgafQe2s8"
   },
   "outputs": [],
   "source": [
    "# def loss_function(real, pred):\n",
    "#     mask = 1 - np.equal(real, 0)\n",
    "#     loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "#     return tf.reduce_mean(loss_)\n",
    "\n",
    "optimizer3 = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxDg8eSbKUhK"
   },
   "outputs": [],
   "source": [
    "# check points\n",
    "\n",
    "checkpoint_dir = './ck_20190209'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer3,\n",
    "                                 model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "MKhZJ9w4e2s_",
    "outputId": "30262be4-f740-4ab5-b565-7b50e3137445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/koohashi/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1 Batch 0 Loss 10.8198\n",
      "Epoch 1 Batch 1000 Loss 3.9865\n",
      "Epoch 1 Loss 170.5722\n",
      "Time taken for 1 epoch 13400.2575507164 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.7296\n",
      "Epoch 2 Batch 1000 Loss 3.0576\n",
      "Epoch 2 Loss 128.7968\n",
      "Time taken for 1 epoch 13533.78256893158 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.2049\n",
      "Epoch 3 Batch 1000 Loss 2.8589\n",
      "Epoch 3 Loss 116.3599\n",
      "Time taken for 1 epoch 13260.481710672379 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.8040\n",
      "Epoch 4 Batch 1000 Loss 2.6882\n",
      "Epoch 4 Loss 109.9504\n",
      "Time taken for 1 epoch 13177.888521194458 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.6604\n",
      "Epoch 5 Batch 1000 Loss 2.5522\n",
      "Epoch 5 Loss 103.8546\n",
      "Time taken for 1 epoch 13548.72274518013 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.5280\n",
      "Epoch 6 Batch 1000 Loss 2.3961\n",
      "Epoch 6 Loss 97.8981\n",
      "Time taken for 1 epoch 13494.05921292305 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.3890\n",
      "Epoch 7 Batch 1000 Loss 2.2799\n",
      "Epoch 7 Loss 92.0714\n",
      "Time taken for 1 epoch 13152.815173149109 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.2569\n",
      "Epoch 8 Batch 1000 Loss 2.1332\n",
      "Epoch 8 Loss 86.5806\n",
      "Time taken for 1 epoch 13541.906535863876 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.1018\n",
      "Epoch 9 Batch 1000 Loss 2.0169\n",
      "Epoch 9 Loss 81.4816\n",
      "Time taken for 1 epoch 13719.180119037628 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.9702\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8c30479cf3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss1_np\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mbackward_function\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    727\u001b[0m               if a is not None and i not in skip_positions]\n\u001b[1;32m    728\u001b[0m       return self._backward_graph_function._call_flat(  # pylint: disable=protected-access\n\u001b[0;32m--> 729\u001b[0;31m           list(args) + side_outputs)\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     tape.record_operation(self._forward_function.signature.name, real_outputs,\n",
      "\u001b[0;32m~/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_call_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_proto_serialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             executor_type=function_call_options.executor_type)\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mpartitioned_call\u001b[0;34m(args, f, tout, executing_eagerly, config, executor_type)\u001b[0m\n\u001b[1;32m   1081\u001b[0m       outputs = gen_functional_ops.stateful_partitioned_call(\n\u001b[1;32m   1082\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m           executor_type=executor_type)\n\u001b[0m\u001b[1;32m   1084\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m       outputs = gen_functional_ops.partitioned_call(\n",
      "\u001b[0;32m~/anaconda3/envs/vui2/lib/python3.6/site-packages/tensorflow/python/ops/gen_functional_ops.py\u001b[0m in \u001b[0;36mstateful_partitioned_call\u001b[0;34m(args, Tout, f, config, config_proto, executor_type, name)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;34m\"StatefulPartitionedCall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"Tout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \"executor_type\", executor_type)\n\u001b[0m\u001b[1;32m    484\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "import time\n",
    " \n",
    "# # Directory where the checkpoints will be saved\n",
    "# checkpoint_dir3 = './training_checkpoints_80000_70'\n",
    "# # Name of the checkpoint files\n",
    "# checkpoint_prefix3 = os.path.join(checkpoint_dir3, \"ckpt_{epoch}\")\n",
    "\n",
    "# Training step\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    \n",
    "    \n",
    "    hidden_f = model.initialize_hidden_state()\n",
    "    # hidden_f = tf.zeros((BATCH_SIZE, hidden_size))\n",
    "    \n",
    "    hidden_b = hidden_f\n",
    "    \n",
    "    loss2 = 0\n",
    "    \n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        \n",
    "        loss1 = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # feeding the hidden state back into the model\n",
    "            # This is the interesting step\n",
    "    \n",
    "            # predictions, hidden_f, _  = model(inp, hidden_f, True)\n",
    "            predictions, hidden_f  = model(inp, hidden_f, True)\n",
    "            # predictions  = model(inp)\n",
    "        \n",
    "            target = tf.expand_dims(target, 2)\n",
    "            loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
    "            loss1 = tf.reduce_mean(loss)\n",
    "            # print(\"loss333: \", loss333, loss1)\n",
    "\n",
    "        loss1_np = float(loss1.numpy())\n",
    "        batch_loss = (loss1_np / int(inp.shape[1]))\n",
    "        \n",
    "        grads = tape.gradient(loss1, model.variables)\n",
    "        optimizer3.apply_gradients(zip(grads, model.variables))\n",
    "        \n",
    "        loss2 = loss2 + batch_loss\n",
    "        \n",
    "        if batch_n % 1000 == 0:\n",
    "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
    "            print(template.format(epoch+1, batch_n, loss1))\n",
    "\n",
    "#     # saving (checkpoint) the model every 5 epochs\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         model.save_weights(checkpoint_prefix3.format(epoch=epoch))\n",
    "        \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss2))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "# model.save_weights(checkpoint_prefix3.format(epoch=epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RMLWUellCHNA",
    "outputId": "e817e2a6-a6fa-45cb-ca2f-655bfd345dd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ck_20190209/ckpt-5'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_weights(checkpoint_prefix3.format(epoch=epoch))\n",
    "\n",
    "checkpoint.save(file_prefix = checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5wcrXPKo4mp"
   },
   "outputs": [],
   "source": [
    "# # weight and model\n",
    "# not working\n",
    "# tf.keras.models.save_model(\n",
    "#     model,\n",
    "#     \"model1111.h5\",\n",
    "# )\n",
    "\n",
    "# not working\n",
    "#model.save(\"model1111.h5\")\n",
    "\n",
    "# looks okay but can not load...\n",
    "# model.save_weights(\"model2222.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4M9lqANeHVTD"
   },
   "outputs": [],
   "source": [
    "# # download  \n",
    "# # files.download( \"model2222.h5\" ) \n",
    "\n",
    "# saving_filename = \"./model2222.h5\"\n",
    "\n",
    "# file_metadata = {\n",
    "#   'name': saving_filename,\n",
    "#   'mimeType': 'application/octet-stream'\n",
    "# }\n",
    "# media = googleapiclient.http.MediaFileUpload(saving_filename, \n",
    "#                         mimetype='application/octet-stream',\n",
    "#                         resumable=True)\n",
    "# created = drive_service.files().create(body=file_metadata,\n",
    "#                                        media_body=media,\n",
    "#                                        fields='id').execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0k-6sTeCHVP1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZepWCtyHVMu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6FvBcyzHVLG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0H1vhS1HVIE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLVZKB07_6VN"
   },
   "outputs": [],
   "source": [
    "# # experiment\n",
    "# # from tf.keras.backend import manual_variable_initialization\n",
    "# tf.keras.backend.manual_variable_initialization(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_UU81DlYF0tR",
    "outputId": "3bbe040d-0b5d-43bc-e09c-192139f041a6"
   },
   "outputs": [],
   "source": [
    "# checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_r_EvV3F1r-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3g7nZBoBZFz"
   },
   "outputs": [],
   "source": [
    "# # download fines\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYosP4v5e2tD"
   },
   "outputs": [],
   "source": [
    "# # download vocabulary \n",
    "# files.download( \"./models/vocabulary.txt\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LSPc7sZgjHYO",
    "outputId": "d4d9b910-c246-4b82-bf78-a8491b2b44fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_checkpoints_80000_70/ckpt-6'"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint_dir3 = \"training_checkpoints_80000_70\"\n",
    "# tf.train.latest_checkpoint(checkpoint_dir3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PazniHdbjHWj"
   },
   "outputs": [],
   "source": [
    "# files.download( \"./training_checkpoints_80000_70/checkpoint\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DPACJ_jjHTZ"
   },
   "outputs": [],
   "source": [
    "# files.download( \"./training_checkpoints_80000_70/ckpt-6.index\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxLBRw9mjHRa"
   },
   "outputs": [],
   "source": [
    "# import google.colab\n",
    "# import googleapiclient.discovery\n",
    "# import googleapiclient.http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noG4mCOhjHQB"
   },
   "outputs": [],
   "source": [
    "# google.colab.auth.authenticate_user()\n",
    "# drive_service = googleapiclient.discovery.build('drive', 'v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXtk-_BujHOf"
   },
   "outputs": [],
   "source": [
    "# saving_filename = \"./training_checkpoints_80000_70/ckpt-6.data-00000-of-00001\"\n",
    "\n",
    "# file_metadata = {\n",
    "#   'name': saving_filename,\n",
    "#   'mimeType': 'application/octet-stream'\n",
    "# }\n",
    "# media = googleapiclient.http.MediaFileUpload(saving_filename, \n",
    "#                         mimetype='application/octet-stream',\n",
    "#                         resumable=True)\n",
    "# created = drive_service.files().create(body=file_metadata,\n",
    "#                                        media_body=media,\n",
    "#                                        fields='id').execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEddlGrbjHLI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjAPFe93jHJd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YexvS7vdjHGf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZQi-KCBDD3R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91sqxp3IDD0j"
   },
   "outputs": [],
   "source": [
    "# load weight and check results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "purswf0FDDo_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6OrRqUVe2tf"
   },
   "outputs": [],
   "source": [
    "# rebuild moel\n",
    "hidden_size = 400\n",
    "embedding_dim = hidden_size\n",
    "vocabulary_size = 50000\n",
    "\n",
    "model = KanaKanjiModel(vocabulary_size, embedding_dim, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJYCmUw0e2ti"
   },
   "outputs": [],
   "source": [
    "# # load model\n",
    "\n",
    "# # Directory where the checkpoints will be saved\n",
    "# checkpoint_dir3 = './training_checkpoints_80000_70'\n",
    "\n",
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zG6ZrfTDER6K",
    "outputId": "58e1ac4d-91cb-4837-9eb2-c3ce108f4849"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ck_20190209/ckpt-5'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir3 = './ck_20190209'\n",
    "tf.train.latest_checkpoint(checkpoint_dir3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pLgiLjcogwwp",
    "outputId": "b09213f0-a16d-4929-d9c8-20af9ad362a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status:  <tensorflow.python.training.checkpointable.util.CheckpointLoadStatus object at 0x1288e8128>\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "# load weight\n",
    "checkpoint_dir = './ck_20190209'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer3,\n",
    "                                 model=model)\n",
    "\n",
    "status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# status = checkpoint.restore(\"./training_checkpoints_ver4/ckpt-3\")\n",
    "print(\"status: \", status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIxSTmtwN2cc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-057rKSe2t9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivTUqGV4e2t_"
   },
   "outputs": [],
   "source": [
    "# # coding: utf-8\n",
    "# hiragana = \"ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろゎわゐゑをん\"\n",
    "\n",
    "# # ひらがなだけの文字列ならTrue\n",
    "# def ishira(strj):\n",
    "#     return all([ch in hiragana for ch in strj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TEywEnVoe2uD",
    "outputId": "c1df9810-8dd9-4565-9ce0-7508f01f9dd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ishira(\"きしゃののりかえ2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmPTjHSOe2uH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNotJvQHe2uJ"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import heapq\n",
    "import operator\n",
    "\n",
    "hiragana_added = []\n",
    "\n",
    "def load_dictionary(model_directory):\n",
    "    vocabulary_path = os.path.join(model_directory, 'vocabulary.txt')\n",
    "    vocabulary = []\n",
    "    for line in open(vocabulary_path):\n",
    "        line = line.rstrip('\\n')\n",
    "        target, source = line.split('/', 1)\n",
    "        vocabulary.append((target, source))\n",
    "\n",
    "    dictionary = collections.defaultdict(list)\n",
    "    for i, (target, source) in enumerate(vocabulary):\n",
    "        dictionary[source].append((target, i))\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def create_lattice(input_, dictionary):\n",
    "    lattice = [[[] for _ in range(len(input_) + 1)] for _ in range(len(input_) + 2)]\n",
    "    _, unk_id = dictionary['_UNK'][0]\n",
    "\n",
    "    for i in range(1, len(input_) + 1):\n",
    "        for j in range(i):\n",
    "            key = input_[j:i]\n",
    "            if key in dictionary:\n",
    "                for target, word_id in dictionary[key]:\n",
    "                    lattice[i][j].append((target, word_id))\n",
    "            elif len(key) == 1:\n",
    "                # Create _UNK node with verbatim target when single character key is not found in the dictionary.\n",
    "                lattice[i][j].append((key, unk_id))\n",
    "\n",
    "    _, eos_id = dictionary['_EOS'][0]\n",
    "    lattice[-1][-1].append(('', eos_id))\n",
    "    return lattice\n",
    "\n",
    "\n",
    "def initialize_queues(lattice, rnn_predictor, dictionary):\n",
    "    # Initialize priority queues for keeping hypotheses\n",
    "    # A hypothesis is a tuple of (cost, string, state, prediction)\n",
    "    # cost is total negative log probability\n",
    "    # state.shape == [hidden_size * layer_size]\n",
    "    # prediction.shape == [vocabulary_size]\n",
    "    \n",
    "    hiragana_added = []\n",
    "    \n",
    "    hidden_f = rnn_predictor.initialize_hidden_state()\n",
    "    hidden_b = hidden_f\n",
    "    \n",
    "    _, bos_id = dictionary['_BOS'][0]\n",
    "    \n",
    "    input_x0 = tf.expand_dims([bos_id], 0)\n",
    "    \n",
    "    # bos_predictions, hidden_f, hidden_b = rnn_predictor(input_x0, hidden_f, hidden_b, False)\n",
    "    # bos_predictions, hidden_f, hidden_b = rnn_predictor(input_x0, hidden_f, False)\n",
    "    bos_predictions, hidden_f = rnn_predictor(input_x0, hidden_f, False)\n",
    "    \n",
    "    \n",
    "    # logits to probability\n",
    "    # bos_predictions = tf.math.softmax(bos_predictions, axis=1)\n",
    "    # print(\"bos_predictions.shape 1: \", bos_predictions.shape)\n",
    "    \n",
    "    bos_predictions = tf.squeeze(bos_predictions, 0)\n",
    "    bos_predictions = tf.squeeze(bos_predictions, 0)\n",
    "    bos_predictions = -1 * tf.nn.log_softmax(bos_predictions, axis=0)\n",
    "    # print(\"bos_predictions.shape 2: \", bos_predictions.shape)\n",
    "    \n",
    "    hidden_f = tf.expand_dims(hidden_f, 0)\n",
    "#     hidden_b = tf.expand_dims(hidden_b, 0)\n",
    "    \n",
    "    # bos_hypothesis = (0.0, '', hidden_f[0], hidden_b[0], bos_predictions[0])\n",
    "    bos_hypothesis = (0.0, '', hidden_f[0], bos_predictions)\n",
    "    queues = [[] for _ in range(len(lattice))]\n",
    "    queues[0].append(bos_hypothesis)\n",
    "    return queues\n",
    "\n",
    "def search(lattice, queues, rnn_predictor, beam_size, viterbi_size):\n",
    "    # Breadth first search with beam pruning and viterbi-like pruning\n",
    "    for i in range(len(lattice)):\n",
    "        queue = []\n",
    "\n",
    "        # create hypotheses without predicting next word\n",
    "        for j in range(len(lattice[i])):\n",
    "            for target, word_id in lattice[i][j]:\n",
    "                \n",
    "                # if word_id == 350:\n",
    "                #    print(\"word_id: \", word_id)\n",
    "                \n",
    "                word_queue = []\n",
    "                for previous_cost, previous_string, previous_state_f, previous_prediction in queues[j]:\n",
    "                    # if logits is bigger, better.\n",
    "                    \n",
    "                    # seems to need give huge priority to first word\n",
    "                    cost = previous_cost + previous_prediction[word_id]\n",
    "#                     if previous_string == \"\":\n",
    "#                         print(\"previous_cost: \", previous_cost)\n",
    "#                         print(\"word_id: \", word_id, previous_prediction[word_id])\n",
    "#                         print(\"previous_prediction.shape: \", previous_prediction)\n",
    "                        \n",
    "#                         cost = previous_cost + previous_prediction[word_id] * 10 * i\n",
    "#                         # cost = previous_cost + previous_prediction[word_id]\n",
    "#                         # if word_id == 2774:\n",
    "#                         #     print(\"word_id: \", word_id, \" cost: \", cost, previous_prediction[word_id])\n",
    "#                     else:\n",
    "#                         cost = previous_cost + previous_prediction[word_id]\n",
    "#                         # if ishira(target) == True:\n",
    "#                         #    cost = previous_cost + previous_prediction[word_id]\n",
    "#                         #else:\n",
    "#                         #    cost = previous_cost + previous_prediction[word_id] * len(target)\n",
    "                        \n",
    "                    # print(\"aa: \", previous_string, target, previous_cost, previous_prediction[word_id].numpy())\n",
    "                    string = previous_string + target\n",
    "                    hypothesis = (cost, string, word_id, previous_state_f)\n",
    "                    word_queue.append(hypothesis)\n",
    "\n",
    "                # if word_id == 350:\n",
    "                #     print(\"word_queue 1: \", word_queue[0].numpy(), word_queue[1] )\n",
    "                    \n",
    "                # prune word_queue to viterbi size\n",
    "                if viterbi_size > 0:\n",
    "                    word_queue = heapq.nsmallest(viterbi_size, word_queue, key=operator.itemgetter(0))\n",
    "                    \n",
    "                queue += word_queue\n",
    "\n",
    "                \n",
    "        # check word id 350\n",
    "#        queue_350 = None\n",
    "#         for item in queue:\n",
    "#             if item[1] == \"近代\":\n",
    "#                 queue_350 = item\n",
    "#                 print(\"found it! 1\")\n",
    "                \n",
    "        # prune queue to beam size\n",
    "        if beam_size > 0:\n",
    "            \n",
    "#             # list all hiragana only strings\n",
    "#             hiragana_word_queue = []\n",
    "#             for item in queue:\n",
    "#                 if ishira(item[1]) == True:\n",
    "#                     hiragana_word_queue.append(item)\n",
    "            \n",
    "            queue = heapq.nsmallest(beam_size, queue, key=operator.itemgetter(0))\n",
    "            \n",
    "#             # add hiragana only strings after pruning\n",
    "#             mojiretu_list = []\n",
    "#             for item in queue:\n",
    "#                 mojiretu_list.append(item[1])\n",
    "\n",
    "#             for item in hiragana_word_queue:\n",
    "#                 if item[1] not in mojiretu_list:\n",
    "#                     if item[1] not in hiragana_added:\n",
    "#                         hiragana_added.append(item[1])\n",
    "#                         queue.append(item)\n",
    "                    \n",
    "            # # add 350 forcibley...\n",
    "            #if queue_350 is not None:\n",
    "            #    queue.append(queue_350)\n",
    "            \n",
    "\n",
    "#         # check word id 350\n",
    "#         for item in queue:\n",
    "#             if item[1] == \"近代\":\n",
    "#                 print(\"found it! 2\")\n",
    "            \n",
    "        # predict next word and state before continue\n",
    "        for cost, string, word_id, previous_state_f in queue:\n",
    "            \n",
    "            input_x0 = tf.expand_dims([word_id], 0)\n",
    "  \n",
    "            # predictions, state_f, state_b = rnn_predictor(input_x0, [previous_state_f], [previous_state_b], False)\n",
    "            # predictions, state_f, state_b = rnn_predictor(input_x0, [previous_state_f], False)\n",
    "            predictions, state_f = rnn_predictor(input_x0, [previous_state_f], False)\n",
    "            # predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "            # logits to probability\n",
    "            # predictions = tf.math.softmax(predictions, axis=1)\n",
    "            \n",
    "            \n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = -1 * tf.nn.log_softmax(predictions, axis=0)\n",
    "            # print(\"predictions.shape: \", predictions.shape)\n",
    "        \n",
    "            state_f = tf.expand_dims(state_f, 0)\n",
    "            # state_b = tf.expand_dims(state_b, 0)\n",
    "            \n",
    "#             hypothesis = (cost, string, state_f[0], predictions[0])\n",
    "            hypothesis = (cost, string, state_f[0], predictions)\n",
    "            queues[i].append(hypothesis)\n",
    "\n",
    "    return queues\n",
    "\n",
    "def decode(source, dictionary, rnn_predictor, beam_size, viterbi_size):\n",
    "    lattice = create_lattice(source, dictionary)\n",
    "    queues = initialize_queues(lattice, rnn_predictor, dictionary)\n",
    "    queues = search(lattice, queues, rnn_predictor, beam_size, viterbi_size)\n",
    "\n",
    "    candidates = []\n",
    "    for cost, string, _, _ in queues[-1]:\n",
    "        candidates.append((string, cost))\n",
    "\n",
    "    top_result = candidates[0][0]\n",
    "    return top_result, candidates, lattice, queues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbKKlSO-e2uL"
   },
   "outputs": [],
   "source": [
    "def aaaaa(line, rnn_predictor):\n",
    "    \n",
    "    # Load settings and vocabulary\n",
    "    model_directory = \"models\"\n",
    "    dictionary = load_dictionary(model_directory)\n",
    "    \n",
    "    # debug\n",
    "    print_nbest = True \n",
    "    print_lattice = True\n",
    "    print_queue = True\n",
    "    \n",
    "    # parameters\n",
    "    beam_size = 5\n",
    "    viterbi_size = 50000\n",
    "    \n",
    "    \n",
    "    # Iterate input file line by line\n",
    "    line = line.rstrip('\\n')\n",
    "\n",
    "    # Decode - this might take ~10 seconds per line\n",
    "    result, candidates, lattice, queues = decode(line, dictionary, rnn_predictor, beam_size, viterbi_size)\n",
    "\n",
    "    # Print decoded results\n",
    "    if not print_nbest:\n",
    "        print(result)\n",
    "    else:\n",
    "        for string, cost in candidates:\n",
    "            print(string, cost)\n",
    "\n",
    "    # Print lattice for debug\n",
    "    if print_lattice:\n",
    "        for i in range(len(lattice)):\n",
    "            for j in range(len(lattice[i])):\n",
    "                print('i = {}, j = {}'.format(i, j))\n",
    "                for target, word_id in lattice[i][j]:\n",
    "                    print(target, word_id)\n",
    "\n",
    "    # Print queues for debug\n",
    "    if print_queue:\n",
    "        for i, queue in enumerate(queues):\n",
    "            print('queue', i)\n",
    "            for cost, string, state_f, prediction in queue:\n",
    "                # print(string, cost)\n",
    "                print(string, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2591
    },
    "colab_type": "code",
    "id": "5oopc1Gde2uN",
    "outputId": "a6ce6042-c11c-402f-8467-d1ca4e86b30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パリの歴史 tf.Tensor(24.277733, shape=(), dtype=float32)\n",
      "パリの歴誌 tf.Tensor(37.341175, shape=(), dtype=float32)\n",
      "パリの歴し tf.Tensor(38.27497, shape=(), dtype=float32)\n",
      "パリの歴史 tf.Tensor(38.383877, shape=(), dtype=float32)\n",
      "パリノ歴史 tf.Tensor(38.963135, shape=(), dtype=float32)\n",
      "i = 0, j = 0\n",
      "i = 0, j = 1\n",
      "i = 0, j = 2\n",
      "i = 0, j = 3\n",
      "i = 0, j = 4\n",
      "i = 0, j = 5\n",
      "i = 0, j = 6\n",
      "i = 1, j = 0\n",
      "パ 17374\n",
      "i = 1, j = 1\n",
      "i = 1, j = 2\n",
      "i = 1, j = 3\n",
      "i = 1, j = 4\n",
      "i = 1, j = 5\n",
      "i = 1, j = 6\n",
      "i = 2, j = 0\n",
      "パリ 1097\n",
      "i = 2, j = 1\n",
      "理 6135\n",
      "り 9986\n",
      "利 12479\n",
      "李 15579\n",
      "リ 30383\n",
      "i = 2, j = 2\n",
      "i = 2, j = 3\n",
      "i = 2, j = 4\n",
      "i = 2, j = 5\n",
      "i = 2, j = 6\n",
      "i = 3, j = 0\n",
      "i = 3, j = 1\n",
      "りの 33680\n",
      "i = 3, j = 2\n",
      "の 2\n",
      "之 7760\n",
      "ノ 14756\n",
      "野 15437\n",
      "乗 42304\n",
      "i = 3, j = 3\n",
      "i = 3, j = 4\n",
      "i = 3, j = 5\n",
      "i = 3, j = 6\n",
      "i = 4, j = 0\n",
      "i = 4, j = 1\n",
      "i = 4, j = 2\n",
      "乗れ 15458\n",
      "i = 4, j = 3\n",
      "れ 14\n",
      "レ 14130\n",
      "i = 4, j = 4\n",
      "i = 4, j = 5\n",
      "i = 4, j = 6\n",
      "i = 5, j = 0\n",
      "i = 5, j = 1\n",
      "i = 5, j = 2\n",
      "i = 5, j = 3\n",
      "暦 7174\n",
      "歴 14874\n",
      "i = 5, j = 4\n",
      "き 84\n",
      "機 247\n",
      "期 342\n",
      "来 931\n",
      "器 1811\n",
      "気 1890\n",
      "木 2089\n",
      "記 3937\n",
      "着 5415\n",
      "黄 7605\n",
      "樹 9280\n",
      "奇 14415\n",
      "伐 25393\n",
      "季 26451\n",
      "貴 29401\n",
      "帰 33916\n",
      "i = 5, j = 5\n",
      "i = 5, j = 6\n",
      "i = 6, j = 0\n",
      "i = 6, j = 1\n",
      "i = 6, j = 2\n",
      "i = 6, j = 3\n",
      "歴史 347\n",
      "i = 6, j = 4\n",
      "騎士 5753\n",
      "棋士 5795\n",
      "岸 8946\n",
      "期し 22363\n",
      "i = 6, j = 5\n",
      "し 13\n",
      "誌 941\n",
      "視 1030\n",
      "死 1176\n",
      "市 1379\n",
      "氏 1645\n",
      "紙 2652\n",
      "史 2689\n",
      "詩 2952\n",
      "師 3035\n",
      "士 5716\n",
      "シ 20394\n",
      "使 24295\n",
      "肢 35270\n",
      "子 39543\n",
      "i = 6, j = 6\n",
      "i = 7, j = 0\n",
      "i = 7, j = 1\n",
      "i = 7, j = 2\n",
      "i = 7, j = 3\n",
      "i = 7, j = 4\n",
      "i = 7, j = 5\n",
      "i = 7, j = 6\n",
      " 4\n",
      "queue 0\n",
      " 0.0\n",
      "queue 1\n",
      "パ tf.Tensor(11.301775, shape=(), dtype=float32)\n",
      "queue 2\n",
      "パリ tf.Tensor(8.32836, shape=(), dtype=float32)\n",
      "パ理 tf.Tensor(24.570126, shape=(), dtype=float32)\n",
      "パ李 tf.Tensor(26.355997, shape=(), dtype=float32)\n",
      "パ利 tf.Tensor(26.3945, shape=(), dtype=float32)\n",
      "パり tf.Tensor(26.560421, shape=(), dtype=float32)\n",
      "queue 3\n",
      "パリの tf.Tensor(9.83942, shape=(), dtype=float32)\n",
      "パリ野 tf.Tensor(22.62117, shape=(), dtype=float32)\n",
      "パリノ tf.Tensor(23.093779, shape=(), dtype=float32)\n",
      "パリ之 tf.Tensor(23.678593, shape=(), dtype=float32)\n",
      "パりの tf.Tensor(28.125753, shape=(), dtype=float32)\n",
      "queue 4\n",
      "パリのれ tf.Tensor(19.513897, shape=(), dtype=float32)\n",
      "パリのレ tf.Tensor(20.851715, shape=(), dtype=float32)\n",
      "パリ乗れ tf.Tensor(27.21105, shape=(), dtype=float32)\n",
      "パリノれ tf.Tensor(32.76809, shape=(), dtype=float32)\n",
      "パリ之レ tf.Tensor(33.255947, shape=(), dtype=float32)\n",
      "queue 5\n",
      "パリの歴 tf.Tensor(20.989296, shape=(), dtype=float32)\n",
      "パリの暦 tf.Tensor(21.75405, shape=(), dtype=float32)\n",
      "パリのれ来 tf.Tensor(28.767403, shape=(), dtype=float32)\n",
      "パリのれ期 tf.Tensor(29.051079, shape=(), dtype=float32)\n",
      "パリのれき tf.Tensor(29.435026, shape=(), dtype=float32)\n",
      "queue 6\n",
      "パリの歴史 tf.Tensor(17.021395, shape=(), dtype=float32)\n",
      "パリノ歴史 tf.Tensor(29.894186, shape=(), dtype=float32)\n",
      "パリの歴誌 tf.Tensor(30.297117, shape=(), dtype=float32)\n",
      "パリの歴し tf.Tensor(31.66509, shape=(), dtype=float32)\n",
      "パリの歴史 tf.Tensor(31.924133, shape=(), dtype=float32)\n",
      "queue 7\n",
      "パリの歴史 tf.Tensor(24.277733, shape=(), dtype=float32)\n",
      "パリの歴誌 tf.Tensor(37.341175, shape=(), dtype=float32)\n",
      "パリの歴し tf.Tensor(38.27497, shape=(), dtype=float32)\n",
      "パリの歴史 tf.Tensor(38.383877, shape=(), dtype=float32)\n",
      "パリノ歴史 tf.Tensor(38.963135, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "aaaaa(\"ぱりのれきし\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xTPs1dtBhLN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kanakanji_convert_3_train.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
